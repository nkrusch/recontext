------------------------------------------------------------------------
INTRODUCTION
------------------------------------------------------------------------

The artifact is an experimental setting for dynamic invariant detection.
Invariant detection finds assertions that hold over all instances of
traced values. The environment is pre-configured with two detectors,
DÉªÉ¢ and Tá´€CÊŸá´‡, and many numerical input traces. The analyzers will have
scalability issues with larger inputs. DÉªÉ¢Uá´˜ is a wrapper for DÉªÉ¢ that
partitions the input trace and yields inference results based on the
partitions. Select parts of the development are verified in Dafny.

Artifact evaluation expectations
 * A smoke test takes about 10 minutes (+ some setup time).
 * Full evaluation takes about 70 min (on an 8-core Linux host).
 * Execution requires Docker (amd or arm).
 * There are no specialty hardware requirements.
 

------------------------------------------------------------------------
SMOKE TEST & FUNCTIONAL EVALUATION
------------------------------------------------------------------------

This section explains how to reproduce the paper claims and complete
a functional evaluation.

The artifact includes:
 * An experimental setup to reproduce paper experiments of Â§3â€“4.
 * All benchmarks, inputs, and source codes used in the experiments.
 * The full verification described in Â§5.


Getting Started Guide
------------------------------------------------------------------------

Prerequisites
 * ğŸ³ Docker - https://docs.docker.com/engine/install
 * ğŸ–¥ï¸ Operating system - any Docker-compatible platform
 * ğŸŒ Internet - only container build requires the host to be online
 * ğŸ§  Memory - the container size is about 1.5GB

â‘  [<2 min] Build the container. On some machines you may need sudo.

    docker build . -t rectx

Alternatively, for a pre-built container, run:

    docker load -i [NAME].tar

â‘¡ Launch the container.

    docker run --rm -v "$(pwd)/rdoc:/rectx/results" -it rectx:latest

The command mounts a shared directory (rdoc) on the host. This way,
the results of all experiments run inside the container will be
immediately visible on the host and persist after container exit.


Source Code Organization
------------------------------------------------------------------------

Except Python packages, all source code is included in the artifact.

     .                          
     â”œâ”€ ğŸ“ dig                 DÉªÉ¢ source code
     â”œâ”€ ğŸ“ digup               source code of our prototype detector
     â”œâ”€ ğŸ“ input/traces        all input traces
     â”œâ”€ ğŸ“ logs                referential result from our experiments
     â”œâ”€ ğŸ“ scripts             helper scripts for running experiments
     â”œâ”€ ğŸ“ tacle               Tá´€CÊŸá´‡ source code
     â”œâ”€ ğŸ“ verified            Dafny-verified codes
     â”œâ”€ Dockerfile             container build script
     â”œâ”€ LICENSE                software license (MIT)
     â”œâ”€ readme.txt             this readme
     â”œâ”€ requirements.txt       Python package dependencies
     â””â”€ *                      other configuration files


Step-by-Step Instructions: Reproducing Paper Claims
------------------------------------------------------------------------

Which claims or results can be replicated:
 * Experiments (Tables 1-4 in Â§3-4, except Â§3.4) and verification (Â§5).

Precisely state the resource requirements you used:
 * see `logs/_host.txt` â†’ Ubuntu 22.04.5, 8 cores, 64GB RAM.

Provide a rough estimate of the experiment times:
 * smoke test 10 min and full evaluation 70 min.
 * The times are based on `logs` and exclude containerization overhead.

Regarding tasks that require a large amount of resources:
 * The experiment of Â§3.4 takes about 16h. Reproduction requires
   only extending the timeout (`make TO=54000`). We do not expect the
   AEC to repeat the experiment, but claim that it is in principle
   reproducible with the artifact (the evaluations commands do execute
   the workloads, but terminate with a timeout).


Checking The Verification (Â§5)
------------------------------

The `verified` directory contains:
 * Data mutation algorithm to maintain invariants under perturbations.
 * Verified benchmarks to confirm consistency with linear invariants.

Check the data mutation verification by running:

    dafny verify verified/mutation.dfy

This should print "finished with 18 verified, 0 errors."

To confirm the development matches the paper description, manually
review the following parts of verified/mutation.dfy.
 * Fig. 4 type definitions: L15â€“28
 * Fig. 5 correctness: L35â€“48 + L75â€“79
 * Fig. 6 mutations: L90â€“105 + L179â€“192

Some identifiers are changed for paper presentation (paper â†’ code):
 * PreservingMutation â†’ ControlledMutation
 * CorrectTraceMutation â†’ MutableHold
 * VectorMutation â†’ VectMutation
 * MutationCorrect â†’ MutableVecCorrect
 * ImmutableCorrect â†’ ImmutableVecCorrect
 * EnsureImmutable â†’ EnsureImmVector
 * EnsureMutation â†’ EnsureMutVector
 * Ïˆ â†’ mutables-type
 * Ï† â†’ immutables-type


Reproducing Experiments (Â§3-4)
------------------------------

[~10 min] A SMOKE TEST

    make TO=60 SZ=25

[~70 min] FULL EVALUATION

    make

The make-command just runs a sequence of other commands.
They generate statistics of traces and host machine, run all
experiments, and generate table plots.

To run the same as individual steps:

    COMMAND        DESCRIPTION                            DURATION
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    make stats     Gather statistics of traces [Table 2]   < 1 min
    make host      Capture host machine details            < 1 min
    make dig       Run DÉªÉ¢ experiments [Tables 1, 3]       ~30 min
    make digup     Run DÉªÉ¢Uá´˜ experiments [Table 3]          ~5 min
    make times     Run exec-time experiments [Table 4]     ~30 min
    make score     Plot all tables                         < 1 min

Run `make clean` to reset `results` directory.

Overridable Makefile options

    OPTION       DESCRIPTION                               DEFAULT
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    OUT          Path to results directory                 results
    SZ           Trace sizes for times experiment     25 50 75 100
    TO           Benchmark timeout in seconds                   90


Matching Experiment Results with Paper (Â§3-4)
---------------------------------------------

The experiment results are written to `results` directory
(if running in Docker, the directory appears as `rdoc` on the host).

Table 1 (first table in _results.txt)
 * NOTE: There is are transcription issues between the paper and the
   results in logs/_results.txt. The logged result is correct.
   The issue does not affect the main experimental claim (that Dig is
   effective at finding invariants) since passing âœ” cases are the same.
 * Symbol ? means SMT solver could not automatically prove equivalence
   between known and inferred invariants (appears as âœ— in paper).

Table 2 ("Invariant benchmarks" in _inputs.txt)
 * The function benchmarks appear in different order.

Table 3 (second table in _results.txt)
 * First 3 rows should match between paper and artifact.
 * "surveillance" in paper â†’ "ds_blink" in artifact.
 * "intrusion" in paper â†’ "ds_lt-fs-id" in artifact.
 * Last 2 rows (wine/*) will be incomplete (require longer timeout).
 * Variable and record counts are in _inputs.txt in "datasets" table.
 * time values are recorded in _log.txt.

Table 4 (third table in _results.txt)
 * Paper and artifact use different units (min/ms).
 * Exact times will vary by hardware, but the relative pattern should
   be observable, e.g., tacle times increase rapidly with input size.


------------------------------------------------------------------------
REUSABILITY GUIDE
------------------------------------------------------------------------

The artifact is reusable in the sense that it can be extended to
process new inputs, beyond the paper.

The reusability claims are:
 * Documentation and packaging facilitate reuse in new environments.
 * Artifact documents dependencies and platform support.
 * Artifact explains how to adapt the setup to new inputs.


Native Execution from Sources
------------------------------------------------------------------------

Platform support
 * YES for Unix-like operating systems:
   tested on Linux 22.04, macOS v11/Intel, and macOS v15 M1/ARM
 * Windows compatibility is untested

Software prerequisites
 * â—¼ï¸ bash   â‰¥3.2:      https://www.gnu.org/software/bash/
 * âš’ï¸ make   â‰¥4:        https://www.gnu.org/software/make/
 * ğŸ Python â‰¥3.10:     https://www.python.org/downloads/
 * ğŸ’› Dafny  â‰¥4.7.0:    https://dafny.org

NOTE: Dafny is only needed for verification and not for experiments;
      this guide will work without Dafny.


â‘  [Optional] At the sources root, create a fresh virtual environment.

    python3 -m venv venv && source venv/bin/activate

For help, the guide for creating virtual environments is at
https://docs.python.org/3/library/venv.html.

â‘¡ Install Python dependencies.

    python3 -m pip install -r requirements.txt

The precise environment we used in the paper experiments is captured in
`req.repro.txt`. It can be used as an alternative source of Python
package installation.


Executing Custom Workloads
------------------------------------------------------------------------

The command format to run an experiment on a single input is

    make results/[INPUT].[EXT]

* [INPUT] is a benchmark name from `input/traces`.
* [EXT] is the choice invariant detector in { dig, digup, tacle }.
* For example `make results/l_003.dig` runs linear problem #3 on DÉªÉ¢.


Adding New Inputs
------------------------------------------------------------------------

The artifact can be extended to new numeric input traces.

This example can be executed in a Docker container or on a native host.
The commands correspond to:
 â‘  create a comma-separated value (CSV) file
 â‘¡ convert the CSV to a trace
 â‘¢ run an experiment and
 â‘£ inspect the result.

    echo "varA,varB
    1,7
    75,8
    5,12
    17,-4" > test.csv

    python3 -m scripts -a trace test.csv > input/traces/test.csv
    make results/test.dig
    cat results/test.dig

As output, you should finally observe the invariants extracted from
the test data.

    trace1 (5 invs):
    1. -varB <= 4
    2. varB <= 12
    3. -varA + varB <= 7
    4. -varA - varB <= -8
    5. varA === 1 (mod 2)


------------------------------------------------------------------------
LICENSING
------------------------------------------------------------------------

* The source code introduced in the artifact is licensed under MIT.
* The UCI datasets at input/traces are licensed under CC BY 4.0.
* The DÉªÉ¢ detector is licensed under MIT and Tá´€CÊŸá´‡ is unlicensed.
